{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad7b4703",
   "metadata": {},
   "source": [
    "# Проект \"Определение уровня английского для фильмов\"\n",
    "\n",
    "В данном проекте необходимо определить, какой уровень английского языка представлен в фильме. Уровень английского языка определяется в соответствии с уровнем Oxford CEFR. Определение уровня осуществляется на основе субтитров к фильму. В дополнение, можно добавлять фильмы и субтитры к ним из открытых источников, поскольку первоначально в дасете 241 фильм.\n",
    "\n",
    "**План работы:**\n",
    "1. [Предобработка данных](#data_preprocessing)\n",
    "2. [Расширение датасета](#dataset_expansion)\n",
    "3. [Выбор метрики](#metric)\n",
    "4. [Создание модели](#model)\n",
    "5. [Анализ результатов](#analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e26840",
   "metadata": {},
   "source": [
    "<a name='data_preprocessing'></a>\n",
    "## 1. Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ed5c45",
   "metadata": {},
   "source": [
    "### 1. 1. Общий файл с фильмами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67aa856e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/midle/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/midle/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Импортирование необходимых библиотек\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import pysrt\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acc9b2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              movie   level\n",
       "0         10_Cloverfield_lane(2016)      B1\n",
       "1  10_things_I_hate_about_you(1999)      B1\n",
       "2              A_knights_tale(2001)      B2\n",
       "3              A_star_is_born(2018)      B2\n",
       "4                     Aladdin(1992)  A2/A2+"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Импорт файла\n",
    "df_movies = pd.read_excel(\n",
    "    '/Users/midle/Desktop/Coding/Data Science/Projects/English_level/English_scores/movies_labels.xlsx',\n",
    ")\n",
    "\n",
    "df_movies.drop('id', axis=1, inplace=True)\n",
    "df_movies.columns = ['movie', 'level']\n",
    "\n",
    "df_movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c0e15b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>level</th>\n",
       "      <th>level_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>B1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>B1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>B2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>B2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>A2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              movie level  level_num\n",
       "0         10_Cloverfield_lane(2016)    B1          1\n",
       "1  10_things_I_hate_about_you(1999)    B1          1\n",
       "2              A_knights_tale(2001)    B2          2\n",
       "3              A_star_is_born(2018)    B2          2\n",
       "4                     Aladdin(1992)    A2          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Заменим промежуточный уровень на нижний\n",
    "df_movies.loc[df_movies['level'] == 'A2/A2+', 'level'] = 'A2'\n",
    "df_movies.loc[df_movies['level'] == 'A2/A2+, B1', 'level'] = 'A2'\n",
    "df_movies.loc[df_movies['level'] == 'B1, B2', 'level'] = 'B1'\n",
    "\n",
    "df_movies['level_num'] = df_movies['level'].map({\n",
    "    'A2': 0,\n",
    "    'B1': 1,\n",
    "    'B2': 2,\n",
    "    'C1': 3\n",
    "})\n",
    "\n",
    "df_movies.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e984c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movie        0\n",
       "level        0\n",
       "level_num    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Пропущенные значения\n",
    "df_movies.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e269ec0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B2    101\n",
       "B1     63\n",
       "C1     40\n",
       "A2     37\n",
       "Name: level, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Распределение количества фильмов по уровням \n",
    "df_movies['level'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430612b7",
   "metadata": {},
   "source": [
    "### 1. 2. Файлы с субтитрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d57ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для загрузки субтитров и их лемматизация\n",
    "def saving_subs(film_loc):\n",
    "    # Загрузка субтитров по кодированию\n",
    "    film_subs = []\n",
    "    encodings = ['', 'UTF-8-SIG', 'ISO-8859-1', 'utf-8', 'Windows-1252', 'ascii']\n",
    "    encoding_number = 0\n",
    "\n",
    "    # Проверка возможности правильности кодирования, иначе - использование другого\n",
    "    while not film_subs:\n",
    "        try:\n",
    "            film_subs = pysrt.open(\n",
    "                film_loc,\n",
    "                encoding=encodings[encoding_number]\n",
    "            )\n",
    "        except UnicodeDecodeError:\n",
    "            encoding_number += 1\n",
    "    \n",
    "    HTML = r'<.*?>' # html тэги меняем на пробел\n",
    "    TAG = r'{.*?}' # тэги меняем на пробел\n",
    "    COMMENTS = r'[\\(\\[][A-Za-z ]+[\\)\\]]' # комменты в скобках меняем на пробел\n",
    "    UPPER = r'[[A-Za-z ]+[\\:\\]]' # указания на того кто говорит (BOBBY:)\n",
    "    LETTERS = r'[^a-zA-Z\\'.,!? ]' # все что не буквы меняем на пробел \n",
    "    SPACES = r'([ ])\\1+' # повторяющиеся пробелы меняем на один пробел\n",
    "    DOTS = r'[\\.]+' # многоточие меняем на точку\n",
    "    SYMB = r\"[^\\w\\d'\\s]\" # знаки препинания кроме апострофа\n",
    "\n",
    "    def clean_subs(subs):\n",
    "        subs = subs[1:] # удаляем первый рекламный субтитр\n",
    "        txt = re.sub(HTML, ' ', subs.text) # html тэги меняем на пробел\n",
    "        txt = re.sub(COMMENTS, ' ', txt) # комменты в скобках меняем на пробел\n",
    "        txt = re.sub(UPPER, ' ', txt) # указания на того кто говорит (BOBBY:)\n",
    "        txt = re.sub(LETTERS, ' ', txt) # все что не буквы меняем на пробел\n",
    "        txt = re.sub(DOTS, r'.', txt) # многоточие меняем на точку\n",
    "        txt = re.sub(SPACES, r'\\1', txt) # повторяющиеся пробелы меняем на один пробел\n",
    "        txt = re.sub(SYMB, '', txt) # знаки препинания кроме апострофа на пустую строку\n",
    "        txt = re.sub('www', '', txt) # кое-где остаётся www, то же меняем на пустую строку\n",
    "        txt = txt.lstrip() # обрезка пробелов слева\n",
    "        txt = txt.encode('ascii', 'ignore').decode() # удаляем все что не ascii символы \n",
    "        txt = txt.lower() # текст в нижний регистр\n",
    "        return txt\n",
    "\n",
    "    # Объединение текста воедино, убирая лишнее\n",
    "    for sub in film_subs:\n",
    "        sub_text = str(sub.text).split('\\n')\n",
    "        try:\n",
    "            for i in range(len(sub_text)):\n",
    "                if not sub_text[i][0] == '(' and not sub_text[i][-1] == ')' and not sub_text[i][0] == '<' \\\n",
    "                and not sub_text[i][-1] == '>' and not sub_text[i][0] == '[' and not sub_text[i][-1] == ']':\n",
    "                    film_text += sub_text[i]\n",
    "                    film_text += ' '\n",
    "        except IndexError: \n",
    "            pass\n",
    "\n",
    "    # Токенизация по словам\n",
    "    film_words_tokens = nltk.word_tokenize(film_text)\n",
    "\n",
    "    numbers_and_symbols = ['.', ',', '!', ':', ';', '?', '_', '__', '...', '#', '--', '``', 'âª', '\\n', 'br/', \\\n",
    "                           '<', '>', '\"', '=', '/', '\\\\', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    film_words = []\n",
    "\n",
    "    # Лемматизация слов и добавление их в отдельный список\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(len(film_words_tokens)):\n",
    "        word = lemmatizer.lemmatize(film_words_tokens[i])\n",
    "        # Проверка на наличие слова в списке\n",
    "        if word not in film_words:\n",
    "            # Использование только слов без цифр и символов\n",
    "            if word.startswith(\"'\"):\n",
    "                film_words.append('to be')\n",
    "            elif word == \"n't\":\n",
    "                film_words.append('not')\n",
    "            elif word == \"y'all\":\n",
    "                film_words.append('you')\n",
    "                film_words.append('all')\n",
    "\n",
    "            if not re.search('\\W', word) and word not in numbers_and_symbols:\n",
    "                film_words.append(word)\n",
    "    \n",
    "    # Объединение слов в одну строчку\n",
    "    film_words = \" \".join(film_words)\n",
    "    return film_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d5f9c7",
   "metadata": {},
   "source": [
    "# Функция для загрузки субтитров и их лемматизация\n",
    "def saving_subs(film_loc):\n",
    "    # Загрузка субтитров по кодированию\n",
    "    film_subs = []\n",
    "    encodings = ['', 'UTF-8-SIG', 'ISO-8859-1', 'utf-8', 'Windows-1252', 'ascii']\n",
    "    encoding_number = 0\n",
    "\n",
    "    # Проверка возможности правильности кодирования, иначе - использование другого\n",
    "    while not film_subs:\n",
    "        try:\n",
    "            film_subs = pysrt.open(\n",
    "                film_loc,\n",
    "                encoding=encodings[encoding_number]\n",
    "            )\n",
    "        except UnicodeDecodeError:\n",
    "            encoding_number += 1\n",
    "\n",
    "    film_text = ''\n",
    "\n",
    "    # Объединение текста воедино, убирая лишнее\n",
    "    for sub in film_subs:\n",
    "        sub_text = str(sub.text).split('\\n')\n",
    "        try:\n",
    "            for i in range(len(sub_text)):\n",
    "                if not sub_text[i][0] == '(' and not sub_text[i][-1] == ')' and not sub_text[i][0] == '<' \\\n",
    "                and not sub_text[i][-1] == '>' and not sub_text[i][0] == '[' and not sub_text[i][-1] == ']':\n",
    "                    film_text += sub_text[i]\n",
    "                    film_text += ' '\n",
    "        except IndexError: \n",
    "            pass\n",
    "\n",
    "    # Токенизация по словам\n",
    "    film_words_tokens = nltk.word_tokenize(film_text)\n",
    "\n",
    "    numbers_and_symbols = ['.', ',', '!', ':', ';', '?', '_', '__', '...', '#', '--', '``', 'âª', '\\n', 'br/', \\\n",
    "                           '<', '>', '\"', '=', '/', '\\\\', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    film_words = []\n",
    "\n",
    "    # Лемматизация слов и добавление их в отдельный список\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(len(film_words_tokens)):\n",
    "        word = lemmatizer.lemmatize(film_words_tokens[i])\n",
    "        # Проверка на наличие слова в списке\n",
    "        if word not in film_words:\n",
    "            # Использование только слов без цифр и символов\n",
    "            if word.startswith(\"'\"):\n",
    "                film_words.append('to be')\n",
    "            elif word == \"n't\":\n",
    "                film_words.append('not')\n",
    "            elif word == \"y'all\":\n",
    "                film_words.append('you')\n",
    "                film_words.append('all')\n",
    "\n",
    "            if not re.search('\\W', word) and word not in numbers_and_symbols:\n",
    "                film_words.append(word)\n",
    "    \n",
    "    # Объединение слов в одну строчку\n",
    "    film_words = \" \".join(film_words)\n",
    "    return film_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85e7ebb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pysrt.srtfile.SubRipFile'>\n",
      "<class 'pysrt.srtfile.SubRipFile'>\n",
      "<class 'pysrt.srtfile.SubRipFile'>\n",
      "<class 'pysrt.srtfile.SubRipFile'>\n",
      "<class 'pysrt.srtfile.SubRipFile'>\n",
      "<class 'pysrt.srtfile.SubRipFile'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1be23bb10b7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdir_found\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0msubs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaving_subs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"/Users/midle/Desktop/Coding/Data Science/Projects/English_level/English_scores/Subtitles_all/{folders[folders_num]}/{row['movie']}.srt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mfolders_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-cc0660ec53e4>\u001b[0m in \u001b[0;36msaving_subs\u001b[0;34m(film_loc)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilm_words_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Проверка на наличие слова в списке\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilm_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0;31m# Использование только слов без цифр и символов\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Получение субтитров для каждого фильма, воспользовавшись функцией для обработки субтитров\n",
    "for index, row in df_movies.iterrows():\n",
    "    folders = ['A2', 'B1', 'B2', 'C1', 'Subtitles']\n",
    "    folders_num = 0\n",
    "    dir_found = False\n",
    "    \n",
    "    while not dir_found:\n",
    "        try:\n",
    "            subs = saving_subs(f\"/Users/midle/Desktop/Coding/Data Science/Projects/English_level/English_scores/Subtitles_all/{folders[folders_num]}/{row['movie']}.srt\")\n",
    "        except FileNotFoundError:\n",
    "            folders_num += 1\n",
    "        except IndexError:\n",
    "            dir_found = True\n",
    "        else:\n",
    "            df_movies.loc[df_movies['movie'] == row['movie'], 'subs'] = subs\n",
    "            dir_found = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf38462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление фильмов, к которым не были найдены субтитры\n",
    "df_movies = df_movies[(df_movies['subs'] != '') & (df_movies['subs'].notna())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff1718c",
   "metadata": {},
   "source": [
    "### 1. 3. Слова по уровню языка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bf23ab",
   "metadata": {},
   "source": [
    "Создадим список со всеми словами, а также отдельный словарь с ключами в виде уровней английского и значениями - индексами начала слов уровня - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4af671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ссылки на слова (А1-C1)\n",
    "a1_b2_link = '/Users/midle/Desktop/Coding/Data Science/Projects/English_level/Oxford_CEFR_level/The_Oxford_3000_by_CEFR_level.pdf'\n",
    "b2_c1_link = '/Users/midle/Desktop/Coding/Data Science/Projects/English_level/Oxford_CEFR_level/The_Oxford_5000_by_CEFR_level.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18d9b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для сохранения слов в список из PDF-файла \n",
    "def get_level_words(link):\n",
    "    reader = PyPDF2.PdfReader(link)\n",
    "    \n",
    "    words = ''\n",
    "    for n in range(len(reader.pages)):\n",
    "        words += reader.pages[n].extract_text()\n",
    "        \n",
    "    words = words.split('\\n')\n",
    "    words = [words[n].split()[0] for n in range(len(words)) if n > 1]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa66156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраним слова в отдельные переменные\n",
    "a1_b2_words = get_level_words(a1_b2_link)\n",
    "b2_c1_words = get_level_words(b2_c1_link)\n",
    "\n",
    "# Внесем некоторые корректировки\n",
    "a1_b2_words[1] = 'a'\n",
    "a1_b2_words.insert(2, 'an')\n",
    "b2_c1_words.remove('3000')\n",
    "b2_c1_words.remove('B2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628ac9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим единый список слов\n",
    "\n",
    "level_words = a1_b2_words\n",
    "level_words.extend(b2_c1_words)\n",
    "level_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a62657",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Создадим словарь с ключами в виде уровня английского и значениями в виде индексов начала слов\n",
    "# данного уровня минус единица. Получается, что конец слов по уровню - значение следующего уровня минус единица.\n",
    "levels = ['A1', 'A2', 'B1', 'B2', 'C1']\n",
    "levels_dict = {level: level_words.index(level) for level in levels}\n",
    "\n",
    "print(levels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90176532",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(level_words[3958:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca8fc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение количества слов по уровням для каждого фильма\n",
    "for index, row in df_movies.iterrows():\n",
    "    # Словарь с подсчетом слов по каждому уровню для отдельного фильма\n",
    "    film_level_words = {level: [] for level in levels}\n",
    "    \n",
    "    # Добавление в словарь слов по их уровням\n",
    "    for word in row['subs'].split():\n",
    "        try:\n",
    "            word_index = level_words.index(word.lower())\n",
    "            if levels_dict['A1'] < word_index < levels_dict['A2']:\n",
    "                if word not in film_level_words['A1']:\n",
    "                    film_level_words['A1'].append(word)\n",
    "            elif levels_dict['A2'] < word_index < levels_dict['B1']:\n",
    "                if word not in film_level_words['A2']:\n",
    "                    film_level_words['A2'].append(word)\n",
    "            elif levels_dict['B1'] < word_index < levels_dict['B2']:\n",
    "                if word not in film_level_words['B1']:\n",
    "                    film_level_words['B1'].append(word)\n",
    "            elif levels_dict['B2'] < word_index < levels_dict['C1']:\n",
    "                if word not in film_level_words['B2']:\n",
    "                    film_level_words['B2'].append(word)\n",
    "            else:\n",
    "                if word not in film_level_words['C1']:\n",
    "                    film_level_words['C1'].append(word)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    # Подсчет количества слов по уровням\n",
    "    levels_words_count = {level: len(film_level_words[level]) for level in levels}\n",
    "    \n",
    "    # Добавления количества слов по уровням в отдельные столбцы\n",
    "    for level in levels:\n",
    "        df_movies.loc[df_movies['movie'] == row['movie'], level] = levels_words_count[level]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62262dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получения значения количества слов по уровням для отдельного фильмы\n",
    "'''\n",
    "movie = df_movies[df_movies['movie'] == 'Suits.S03E06.720p.HDTV.x264-mSD']\n",
    "total_words = movie[levels].sum(axis=1)\n",
    "words_by_previous_levels = 0\n",
    "\n",
    "for level in levels:    \n",
    "    if ((movie[level].values + words_by_previous_levels) / total_words >= 0.75).values[0]:\n",
    "        print(level)\n",
    "        break\n",
    "    else:\n",
    "        words_by_previous_levels += movie[level].values\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e6440d",
   "metadata": {},
   "source": [
    "<a name='dataset_expansion'></a>\n",
    "## 2. Расширение датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22907a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62e668a0",
   "metadata": {},
   "source": [
    "<a name='metric'></a>\n",
    "## 3. Выбор метрики"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c963860",
   "metadata": {},
   "source": [
    "Имеем дело с задачей мультиклассификации. В связи с этим необходимо использовать соответствующие метрики определения качества моделей. Назначение проекта - помочь учащимся, изучающих английский язык. В связи с этим лучше всего использовать F1-меру, поскольку она сочетает в себе и точность (precision), и полноту (recall). Однако, поскольку мы имеем дело с мультиклассификацией, то необходимо воспользоваться \"Макро F1-мерой\" (деомнстрирующей среднее значение F1-меры по классам) и \"Микро F1-мерой\" (глобальная средняя F1-меры, вычисляющая сумму TP, FN и FP).\n",
    "\n",
    "Таким образом, для определения качества моделей будем использовать **f1_micro** и **f1_macro**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef910c7a",
   "metadata": {},
   "source": [
    "<a name='model'></a>\n",
    "## 4. Создание модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df6432d",
   "metadata": {},
   "source": [
    "### 4. 1. Разделение выборок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2086fe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение датасета на тренировочную и тестовую выборки\n",
    "\n",
    "# X = df_movies.drop(['movie', 'level', 'level_num'], axis=1)\n",
    "X = df_movies['subs']\n",
    "y = df_movies['level_num']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df_movies['level_num']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c389af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для нахождения предсказаний\n",
    "def get_y_pred(model):\n",
    "    vectorizer = TfidfVectorizer(stop_words={'english'})\n",
    "    vec_col = ['subs']\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "#     preprocessing = ColumnTransformer([\n",
    "#         ('vectorizer', vectorizer, vec_col),\n",
    "#         ('scaler', scaler, levels),\n",
    "#     ])\n",
    "    \n",
    "#     pipe = Pipeline([\n",
    "#         ('preprocessing', preprocessing),\n",
    "#         ('model', model)\n",
    "#     ])\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('vectorizer', vectorizer),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff69afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_f1_score(model, pred):\n",
    "    # Проверка DecisionTreeClassifier на тестовой выборке\n",
    "    print(f\"{model}:\")\n",
    "    print(\"\\tf1_micro:\", f1_score(y_test, pred, average='micro'))\n",
    "    print(\"\\tf1_macro:\", f1_score(y_test, pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444b49ac",
   "metadata": {},
   "source": [
    "### 4. 2. DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f555efda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(random_state=42)\n",
    "y_pred = get_y_pred(model)\n",
    "print_f1_score(model, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df939758",
   "metadata": {},
   "source": [
    "### 4. 3. RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4c8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state=42)\n",
    "y_pred = get_y_pred(model)\n",
    "print_f1_score(model, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f446f7",
   "metadata": {},
   "source": [
    "### 4. 4. LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9e1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=300)\n",
    "y_pred = get_y_pred(model)\n",
    "print_f1_score(model, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396ab165",
   "metadata": {},
   "source": [
    "<a name='analysis'></a>\n",
    "## 5. Анализ результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8498d0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4c3954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da032e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f744315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faefcc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
