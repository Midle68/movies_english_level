{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad7b4703",
   "metadata": {},
   "source": [
    "# Проект \"Определение уровня английского для фильмов\"\n",
    "\n",
    "В данном проекте необходимо определить, какой уровень английского языка представлен в фильме. Уровень английского языка определяется в соответствии с уровнем Oxford CEFR. Определение уровня осуществляется на основе субтитров к фильму. В дополнение, можно добавлять фильмы и субтитры к ним из открытых источников, поскольку первоначально в дасете 241 фильм.\n",
    "\n",
    "**План работы:**\n",
    "1. [Предобработка данных](#data_preprocessing)\n",
    "2. [Расширение датасета](#dataset_expansion)\n",
    "3. [Выбор метрики](#metric)\n",
    "4. [Создание модели](#model)\n",
    "5. [Анализ результатов](#analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e26840",
   "metadata": {},
   "source": [
    "<a name='data_preprocessing'></a>\n",
    "## 1. Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ed5c45",
   "metadata": {},
   "source": [
    "### 1. 1. Общий файл с фильмами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "67aa856e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/midle/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/midle/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Импортирование необходимых библиотек\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import pysrt\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ff55f654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер: (499, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              movie   level\n",
       "0         10_Cloverfield_lane(2016)      B1\n",
       "1  10_things_I_hate_about_you(1999)      B1\n",
       "2              A_knights_tale(2001)      B2\n",
       "3              A_star_is_born(2018)      B2\n",
       "4                     Aladdin(1992)  A2/A2+"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Импорт файла\n",
    "df_movies = pd.read_excel('movies_labels.xlsx')\n",
    "\n",
    "df_movies.drop('id', axis=1, inplace=True)\n",
    "df_movies.columns = ['movie', 'level']\n",
    "\n",
    "print('Размер:', df_movies.shape)\n",
    "df_movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4c0e15b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>level</th>\n",
       "      <th>level_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>B1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>B1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>B2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>B2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>A2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              movie level  level_num\n",
       "0         10_Cloverfield_lane(2016)    B1          2\n",
       "1  10_things_I_hate_about_you(1999)    B1          2\n",
       "2              A_knights_tale(2001)    B2          3\n",
       "3              A_star_is_born(2018)    B2          3\n",
       "4                     Aladdin(1992)    A2          1"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Замена промежуточный уровень на нижний\n",
    "df_movies.loc[df_movies['level'] == 'A2/A2+', 'level'] = 'A2'\n",
    "df_movies.loc[df_movies['level'] == 'A2/A2+, B1', 'level'] = 'A2'\n",
    "df_movies.loc[df_movies['level'] == 'B1, B2', 'level'] = 'B1'\n",
    "\n",
    "df_movies['level_num'] = df_movies['level'].map({\n",
    "    'A1': 0,\n",
    "    'A2': 1,\n",
    "    'B1': 2,\n",
    "    'B2': 3,\n",
    "    'C1': 4\n",
    "})\n",
    "\n",
    "df_movies.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "1e984c69",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movie        0\n",
       "level        0\n",
       "level_num    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Пропущенные значения\n",
    "df_movies.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e269ec0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A2    177\n",
       "B2    129\n",
       "B1     92\n",
       "C1     58\n",
       "A1     43\n",
       "Name: level, dtype: int64"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Распределение количества фильмов по уровням \n",
    "df_movies['level'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430612b7",
   "metadata": {},
   "source": [
    "### 1. 2. Файлы с субтитрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1547cebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычленение то, что не относится к словам\n",
    "HTML = r'<.*?>'\n",
    "TAG = r'{.*?}'\n",
    "COMMENTS = r'[\\(\\[][A-Za-z ]+[\\)\\]]'\n",
    "UPPER = r'[[A-Za-z ]+[\\:\\]]'\n",
    "LETTERS = r'[^a-zA-Z\\'.,!? ]'\n",
    "SPACES = r'([ ])\\1+'\n",
    "DOTS = r'[\\.]+'\n",
    "SYMB = r\"[^\\w\\d'\\s]\"\n",
    "\n",
    "# Функция для загрузки и лемматизации субтитров\n",
    "def saving_subs(film_loc):\n",
    "    # Загрузка субтитров по кодированию\n",
    "    film_subs = []\n",
    "    encodings = ['', 'UTF-8-SIG', 'ISO-8859-1', 'utf-8', 'Windows-1252', 'ascii']\n",
    "    encoding_number = 0\n",
    "\n",
    "    # Проверка возможности правильности кодирования, иначе - использование другого\n",
    "    while not film_subs:\n",
    "        try:\n",
    "            film_subs = pysrt.open(\n",
    "                film_loc,\n",
    "                encoding=encodings[encoding_number]\n",
    "            )\n",
    "        except UnicodeDecodeError:\n",
    "            encoding_number += 1\n",
    "\n",
    "    # Удаление лишнего и оставление только слов\n",
    "    fiml_subs = film_subs[1:]\n",
    "    text = re.sub(HTML, ' ', film_subs.text)\n",
    "    text = re.sub(TAG, ' ', text)\n",
    "    text = re.sub(COMMENTS, ' ', text)\n",
    "    text = re.sub(UPPER, ' ', text)\n",
    "    text = re.sub(LETTERS, ' ', text)\n",
    "    text = re.sub(DOTS, r'.', text)\n",
    "    text = re.sub(SPACES, r'\\1', text)\n",
    "    text = re.sub(SYMB, '', text)\n",
    "    text = re.sub('www', '', text)\n",
    "    text = text.lstrip()\n",
    "    text = text.encode('ascii', 'ignore').decode()\n",
    "    text = text.lower()\n",
    "\n",
    "    # Лемматизация слов и добавление их в отдельный список\n",
    "    film_words = []\n",
    "    text_list = text.split()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for i in range(len(text_list)):\n",
    "        word = lemmatizer.lemmatize(text.split()[i])\n",
    "        # Проверка на наличие слова в списке\n",
    "        if word not in film_words:\n",
    "            film_words.append(word)\n",
    "\n",
    "    # Объединение слов в одну строчку\n",
    "    film_words = \" \".join(film_words)\n",
    "    return film_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "22aed415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n"
     ]
    }
   ],
   "source": [
    "# Получение субтитров для каждого фильма, воспользовавшись функцией для обработки субтитров\n",
    "for index, row in df_movies.iterrows():\n",
    "    print(index)\n",
    "    try:\n",
    "        subs = saving_subs(f\"subtitles/{row['movie']}.srt\")\n",
    "        df_movies.loc[df_movies['movie'] == row['movie'], 'subs'] = subs\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "cf38462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление фильмов, к которым не были найдены субтитры\n",
    "df_movies = df_movies[(df_movies['subs'] != '') & (df_movies['subs'].notna())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff1718c",
   "metadata": {},
   "source": [
    "### 1. 3. Слова по уровню языка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bf23ab",
   "metadata": {},
   "source": [
    "Создадим список со всеми словами, а также отдельный словарь с ключами в виде уровней английского и значениями - индексами начала слов уровня - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da4af671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ссылки на слова (А1-C1)\n",
    "a1_b2_link = '/Users/midle/Desktop/Coding/Data Science/Projects/English_level/Oxford_CEFR_level/The_Oxford_3000_by_CEFR_level.pdf'\n",
    "b2_c1_link = '/Users/midle/Desktop/Coding/Data Science/Projects/English_level/Oxford_CEFR_level/The_Oxford_5000_by_CEFR_level.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e18d9b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для сохранения слов в список из PDF-файла \n",
    "def get_level_words(link):\n",
    "    reader = PyPDF2.PdfReader(link)\n",
    "    \n",
    "    words = ''\n",
    "    for n in range(len(reader.pages)):\n",
    "        words += reader.pages[n].extract_text()\n",
    "        \n",
    "    words = words.split('\\n')\n",
    "    words = [words[n].split()[0] for n in range(len(words)) if n > 1]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa66156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение слов по уровням в отдельные переменные\n",
    "a1_b2_words = get_level_words(a1_b2_link)\n",
    "b2_c1_words = get_level_words(b2_c1_link)\n",
    "\n",
    "# Внесем некоторые корректировки\n",
    "a1_b2_words[1] = 'a'\n",
    "a1_b2_words.insert(2, 'an')\n",
    "b2_c1_words.remove('3000')\n",
    "b2_c1_words.remove('B2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "628ac9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A1',\n",
       " 'a',\n",
       " 'an',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'action',\n",
       " 'activity',\n",
       " 'actor',\n",
       " 'actress']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Объединим слова\n",
    "level_words = a1_b2_words\n",
    "level_words.extend(b2_c1_words)\n",
    "level_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26a62657",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A1': 0, 'A2': 891, 'B1': 1752, 'B2': 2550, 'C1': 3958}\n"
     ]
    }
   ],
   "source": [
    "# Создание словаря с ключами в виде уровня английского и значениями в виде индексов начала слов\n",
    "# данного уровня минус единица\n",
    "levels = ['A1', 'A2', 'B1', 'B2', 'C1']\n",
    "levels_dict = {level: level_words.index(level) for level in levels}\n",
    "\n",
    "print(levels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2ca8fc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Не улучшает качество моделей'''\n",
    "\n",
    "'''\n",
    "# Определение количества слов по уровням для каждого фильма\n",
    "for index, row in df_movies.iterrows():\n",
    "    # Словарь с подсчетом слов по каждому уровню для отдельного фильма\n",
    "    film_level_words = {level: [] for level in levels}\n",
    "    \n",
    "    # Добавление в словарь слов по их уровням\n",
    "    for word in row['subs'].split():\n",
    "        try:\n",
    "            word_index = level_words.index(word.lower())\n",
    "            if levels_dict['A1'] < word_index < levels_dict['A2']:\n",
    "                if word not in film_level_words['A1']:\n",
    "                    film_level_words['A1'].append(word)\n",
    "            elif levels_dict['A2'] < word_index < levels_dict['B1']:\n",
    "                if word not in film_level_words['A2']:\n",
    "                    film_level_words['A2'].append(word)\n",
    "            elif levels_dict['B1'] < word_index < levels_dict['B2']:\n",
    "                if word not in film_level_words['B1']:\n",
    "                    film_level_words['B1'].append(word)\n",
    "            elif levels_dict['B2'] < word_index < levels_dict['C1']:\n",
    "                if word not in film_level_words['B2']:\n",
    "                    film_level_words['B2'].append(word)\n",
    "            else:\n",
    "                if word not in film_level_words['C1']:\n",
    "                    film_level_words['C1'].append(word)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    # Подсчет количества слов по уровням\n",
    "    levels_words_count = {level: len(film_level_words[level]) for level in levels}\n",
    "    \n",
    "    # Добавление количества слов по уровням в отдельные столбцы\n",
    "    total_words = sum(levels_words_count.values())\n",
    "    for level in levels:\n",
    "        df_movies.loc[df_movies['movie'] == row['movie'], level] = levels_words_count[level] / total_words\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e668a0",
   "metadata": {},
   "source": [
    "<a name='metric'></a>\n",
    "## 2. Выбор метрики"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c963860",
   "metadata": {},
   "source": [
    "Имеем дело с задачей мультиклассификации. В связи с этим необходимо использовать соответствующие метрики определения качества моделей. Назначение проекта - помочь учащимся, изучающих английский язык. В связи с этим лучше всего использовать F1-меру, поскольку она сочетает в себе и точность (precision), и полноту (recall). Однако, поскольку мы имеем дело с мультиклассификацией, то необходимо воспользоваться \"Макро F1-мерой\" (деомнстрирующей среднее значение F1-меры по классам) и \"Микро F1-мерой\" (глобальная средняя F1-меры, вычисляющая сумму TP, FN и FP).\n",
    "\n",
    "Таким образом, для определения качества моделей будем использовать **f1_micro** и **f1_macro**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef910c7a",
   "metadata": {},
   "source": [
    "<a name='model'></a>\n",
    "## 3. Создание модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df6432d",
   "metadata": {},
   "source": [
    "### 3. 1. Разделение выборок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "2086fe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение датасета на тренировочную и тестовую выборки\n",
    "\n",
    "X = df_movies['subs']\n",
    "y = df_movies['level_num']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df_movies['level_num']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e4c389af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для нахождения предсказаний\n",
    "def get_y_pred(model):\n",
    "    vectorizer = TfidfVectorizer(stop_words={'english'})\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "#     preprocessing = ColumnTransformer([\n",
    "#         ('vectorizer', vectorizer, 'subs'),\n",
    "#         ('scaler', scaler, levels),\n",
    "#     ])\n",
    "    \n",
    "#     pipe = Pipeline([\n",
    "#         ('preprocessing', preprocessing),\n",
    "#         ('model', model)\n",
    "#     ])\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('vectorizer', vectorizer),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "7ff69afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_f1_score(model, pred):\n",
    "    # Проверка DecisionTreeClassifier на тестовой выборке\n",
    "    print(f\"{model}:\")\n",
    "    print(\"\\tf1_micro: {:.4f}\".format(f1_score(y_test, pred, average='micro')))\n",
    "    print(\"\\tf1_macro: {:.4f}\".format(f1_score(y_test, pred, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444b49ac",
   "metadata": {},
   "source": [
    "### 3. 2. DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "ed4bcd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(random_state=42):\n",
      "\tf1_micro: 0.5859\n",
      "\tf1_macro: 0.5516\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=42, )\n",
    "y_pred = get_y_pred(model)\n",
    "print_f1_score(model, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0ba8e030",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel = DecisionTreeClassifier(random_state=42)\\nparams = {\\n    'model__max_depth': range(2, 21, 2),\\n    'model__criterion': ['gini', 'entropy']\\n}\\n\\npreprocessing = ColumnTransformer([\\n    ('vectorizer', vectorizer, 'subs'),\\n    ('scaler', scaler, levels),\\n])\\n\\npipe = Pipeline([\\n    ('preprocessing', preprocessing),\\n    ('model', model)\\n])\\n\\ngrid = GridSearchCV(pipe, params, scoring=scoring, refit='f1_micro')\\ngrid.fit(X_train, y_train)\\n\\nmetrics_cols = [f'mean_test_{x}' for x in scoring]\\nfinal_metrics = pd.DataFrame(grid.cv_results_)[metrics_cols].iloc[grid.best_index_]\\nprint(grid.best_estimator_)\\nprint(final_metrics)\\n\""
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Использование GridSearchCV для DecisionTreeClassifier\n",
    "\n",
    "'''\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "params = {\n",
    "    'model__max_depth': range(2, 21, 2),\n",
    "    'model__criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('vectorizer', vectorizer, 'subs'),\n",
    "    ('scaler', scaler, levels),\n",
    "])\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "grid = GridSearchCV(pipe, params, scoring=scoring, refit='f1_micro')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "metrics_cols = [f'mean_test_{x}' for x in scoring]\n",
    "final_metrics = pd.DataFrame(grid.cv_results_)[metrics_cols].iloc[grid.best_index_]\n",
    "print(grid.best_estimator_)\n",
    "print(final_metrics)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "f555efda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(criterion='entropy', max_depth=20, random_state=42):\n",
      "\tf1_micro: 0.6565656565656566\n",
      "\tf1_macro: 0.6514898753585331\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=42, max_depth=20, criterion='entropy')\n",
    "y_pred = get_y_pred(model)\n",
    "print_f1_score(model, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df939758",
   "metadata": {},
   "source": [
    "### 3. 3. RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "7ee4c8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(max_depth=20, n_estimators=350, random_state=42):\n",
      "\tf1_micro: 0.6768\n",
      "\tf1_macro: 0.5737\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=42, n_estimators=350, max_depth=20)\n",
    "y_pred = get_y_pred(model)\n",
    "print_f1_score(model, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f446f7",
   "metadata": {},
   "source": [
    "### 3. 4. LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "15e9e1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=35, max_iter=300, random_state=42):\n",
      "\tf1_micro: 0.7677\n",
      "\tf1_macro: 0.7177\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=42, max_iter=300, C=35)\n",
    "y_pred = get_y_pred(model)\n",
    "print_f1_score(model, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8652e7c",
   "metadata": {},
   "source": [
    "### 3. 5. RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "4ac8dfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RidgeClassifier(alpha=0.1, random_state=42):\n",
      "\tf1_micro: 0.7879\n",
      "\tf1_macro: 0.7646\n"
     ]
    }
   ],
   "source": [
    "model = RidgeClassifier(random_state=42, alpha=0.1)\n",
    "y_pred = get_y_pred(model)\n",
    "print_f1_score(model, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e4e645",
   "metadata": {},
   "source": [
    "### 3. 6. SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "59fda128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(l1_ratio=0.01, loss='huber', penalty='elasticnet',\n",
      "              random_state=42):\n",
      "\tf1_micro: 0.7879\n",
      "\tf1_macro: 0.7754\n"
     ]
    }
   ],
   "source": [
    "model = SGDClassifier(random_state=42, loss='huber', alpha=0.0001, penalty='elasticnet', l1_ratio=0.01)\n",
    "y_pred = get_y_pred(model)\n",
    "print_f1_score(model, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c9fe22",
   "metadata": {},
   "source": [
    "### 3. 7. LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "568e0bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=3, random_state=42):\n",
      "\tf1_micro: 0.7879\n",
      "\tf1_macro: 0.7646\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVC(random_state=42, C=3)\n",
    "y_pred = get_y_pred(model)\n",
    "print_f1_score(model, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396ab165",
   "metadata": {},
   "source": [
    "<a name='analysis'></a>\n",
    "## 4. Анализ результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8498d0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c5f124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d96f9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaa1379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724a64cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
