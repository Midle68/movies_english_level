{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad7b4703",
   "metadata": {},
   "source": [
    "<a name='introduction'></a>\n",
    "# Проект \"Определение уровня английского для фильмов\"\n",
    "\n",
    "В данном проекте необходимо определить, какой уровень английского языка представлен в фильме. Уровень английского языка определяется в соответствии с уровнем Oxford CEFR. Определение уровня осуществляется на основе субтитров к фильму. В дополнение, можно добавлять фильмы и субтитры к ним из открытых источников, поскольку первоначально в дасете 241 фильм.\n",
    "\n",
    "**План работы:**\n",
    "1. [Предобработка данных](#data_preprocessing)\n",
    "2. [Выбор метрики](#metric)\n",
    "3. [Создание модели](#model)\n",
    "4. [Анализ результатов](#analysis)\n",
    "5. [Сохранение модели](#model_saving)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e26840",
   "metadata": {},
   "source": [
    "<a name='data_preprocessing'></a>\n",
    "## 1. Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ed5c45",
   "metadata": {},
   "source": [
    "### 1. 1. Общий файл с фильмами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67aa856e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/midle/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/midle/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/midle/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/midle/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/midle/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/midle/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Импортирование необходимых библиотек\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import pysrt\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e97b8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер: (499, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              movie   level\n",
       "0         10_Cloverfield_lane(2016)      B1\n",
       "1  10_things_I_hate_about_you(1999)      B1\n",
       "2              A_knights_tale(2001)      B2\n",
       "3              A_star_is_born(2018)      B2\n",
       "4                     Aladdin(1992)  A2/A2+"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Импорт файла\n",
    "df_movies = pd.read_excel('movies_labels.xlsx')\n",
    "\n",
    "df_movies.drop('id', axis=1, inplace=True)\n",
    "df_movies.columns = ['movie', 'level']\n",
    "\n",
    "print('Размер:', df_movies.shape)\n",
    "df_movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c0e15b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>level</th>\n",
       "      <th>level_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>B1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>B1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>B2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>B2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>A2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              movie level  level_num\n",
       "0         10_Cloverfield_lane(2016)    B1          2\n",
       "1  10_things_I_hate_about_you(1999)    B1          2\n",
       "2              A_knights_tale(2001)    B2          3\n",
       "3              A_star_is_born(2018)    B2          3\n",
       "4                     Aladdin(1992)    A2          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Замена промежуточный уровень на нижний\n",
    "df_movies.loc[df_movies['level'] == 'A2/A2+', 'level'] = 'A2'\n",
    "df_movies.loc[df_movies['level'] == 'A2/A2+, B1', 'level'] = 'A2'\n",
    "df_movies.loc[df_movies['level'] == 'B1, B2', 'level'] = 'B1'\n",
    "\n",
    "df_movies['level_num'] = df_movies['level'].map({\n",
    "    'A1': 0,\n",
    "    'A2': 1,\n",
    "    'B1': 2,\n",
    "    'B2': 3,\n",
    "    'C1': 4\n",
    "})\n",
    "\n",
    "df_movies.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e984c69",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movie        0\n",
       "level        0\n",
       "level_num    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Пропущенные значения\n",
    "df_movies.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e269ec0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A2    177\n",
       "B2    129\n",
       "B1     92\n",
       "C1     58\n",
       "A1     43\n",
       "Name: level, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Распределение количества фильмов по уровням \n",
    "df_movies['level'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430612b7",
   "metadata": {},
   "source": [
    "### 1. 2. Файлы с субтитрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1547cebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычленение то, что не относится к словам\n",
    "HTML = r'<.*?>'\n",
    "TAG = r'{.*?}'\n",
    "COMMENTS = r'[\\(\\[][A-Za-z ]+[\\)\\]]'\n",
    "UPPER = r'[[A-Za-z ]+[\\:\\]]'\n",
    "LETTERS = r'[^a-zA-Z\\'.,!? ]'\n",
    "SPACES = r'([ ])\\1+'\n",
    "DOTS = r'[\\.]+'\n",
    "SYMB = r\"[^\\w\\d'\\s]\"\n",
    "\n",
    "\n",
    "# Функция для загрузки и лемматизации субтитров\n",
    "def saving_subs(film_loc):\n",
    "    # Загрузка субтитров по кодированию\n",
    "    film_subs = []\n",
    "    encodings = ['', 'UTF-8-SIG', 'ISO-8859-1', 'utf-8', 'Windows-1252', 'ascii']\n",
    "    encoding_number = 0\n",
    "\n",
    "    # Проверка возможности правильности кодирования, иначе - использование другого\n",
    "    while not film_subs:\n",
    "        try:\n",
    "            film_subs = pysrt.open(\n",
    "                film_loc,\n",
    "                encoding=encodings[encoding_number]\n",
    "            )\n",
    "        except UnicodeDecodeError:\n",
    "            encoding_number += 1\n",
    "    \n",
    "    # Удаление лишнего и оставление только слов\n",
    "    text = re.sub(HTML, ' ', film_subs[1:].text)\n",
    "    text = re.sub(TAG, ' ', text)\n",
    "    text = re.sub(COMMENTS, ' ', text)\n",
    "    text = re.sub(UPPER, ' ', text)\n",
    "    text = re.sub(LETTERS, ' ', text)\n",
    "    text = re.sub(DOTS, r'.', text)\n",
    "    text = re.sub(SPACES, r'\\1', text)\n",
    "    text = re.sub(SYMB, '', text)\n",
    "    text = re.sub('www', '', text)\n",
    "    text = text.lstrip()\n",
    "    text = text.encode('ascii', 'ignore').decode()\n",
    "    text = text.lower()\n",
    "\n",
    "    # Лемматизация слов и добавление их в отдельный список\n",
    "    film_words = []\n",
    "    text_list = text.split()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for i in range(len(text_list)):\n",
    "        word = lemmatizer.lemmatize(text_list[i])\n",
    "        # Проверка на наличие слова в списке и отсутствия в списке имен собственных\n",
    "        if word not in film_words:\n",
    "            film_words.append(word)\n",
    "\n",
    "    # Объединение слов в одну строчку\n",
    "    film_words = \" \".join(film_words)\n",
    "    return film_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd9371d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-5be5d7e96bc6>:33: FutureWarning: Possible nested set at position 1\n",
      "  text = re.sub(UPPER, ' ', text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n"
     ]
    }
   ],
   "source": [
    "# Получение субтитров для каждого фильма, воспользовавшись функцией для обработки субтитров\n",
    "for index, row in df_movies.iterrows():\n",
    "    print(index)\n",
    "    try:\n",
    "        subs = saving_subs(f\"subtitles/{row['movie']}.srt\")\n",
    "        df_movies.loc[df_movies['movie'] == row['movie'], 'subs'] = subs\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf38462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление фильмов, к которым не были найдены субтитры\n",
    "df_movies = df_movies[(df_movies['subs'] != '') & (df_movies['subs'].notna())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff1718c",
   "metadata": {},
   "source": [
    "### 1. 3. Слова по уровню языка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bf23ab",
   "metadata": {},
   "source": [
    "Создадим список со всеми словами, а также отдельный словарь с ключами в виде уровней английского и значениями - индексами начала слов уровня - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da4af671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ссылки на слова (А1-C1)\n",
    "a1_b2_link = 'Oxford_CEFR_level/The_Oxford_3000_by_CEFR_level.pdf'\n",
    "b2_c1_link = 'Oxford_CEFR_level/The_Oxford_5000_by_CEFR_level.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e18d9b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для сохранения слов в список из PDF-файла \n",
    "def get_level_words(link):\n",
    "    reader = PyPDF2.PdfReader(link)\n",
    "    \n",
    "    words = ''\n",
    "    for n in range(len(reader.pages)):\n",
    "        words += reader.pages[n].extract_text()\n",
    "        \n",
    "    words = words.split('\\n')\n",
    "    words = [words[n].split()[0] for n in range(len(words)) if n > 1]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa66156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение слов по уровням в отдельные переменные\n",
    "a1_b2_words = get_level_words(a1_b2_link)\n",
    "b2_c1_words = get_level_words(b2_c1_link)\n",
    "\n",
    "# Внесем некоторые корректировки\n",
    "a1_b2_words[1] = 'a'\n",
    "a1_b2_words.insert(2, 'an')\n",
    "b2_c1_words.remove('3000')\n",
    "b2_c1_words.remove('B2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "628ac9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A1',\n",
       " 'a',\n",
       " 'an',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'action',\n",
       " 'activity',\n",
       " 'actor',\n",
       " 'actress']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Объединим слова\n",
    "level_words = a1_b2_words\n",
    "level_words.extend(b2_c1_words)\n",
    "level_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26a62657",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A1': 0, 'A2': 891, 'B1': 1752, 'B2': 2550, 'C1': 3958}\n"
     ]
    }
   ],
   "source": [
    "# Создание словаря с ключами в виде уровня английского и значениями в виде индексов начала слов\n",
    "# данного уровня минус единица\n",
    "levels = ['A1', 'A2', 'B1', 'B2', 'C1']\n",
    "levels_dict = {level: level_words.index(level) for level in levels}\n",
    "\n",
    "print(levels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ca8fc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Не улучшает качество моделей!\n",
    "\n",
    "\n",
    "# # Определение количества слов по уровням для каждого фильма\n",
    "# for index, row in df_movies.iterrows():\n",
    "#     # Словарь с подсчетом слов по каждому уровню для отдельного фильма\n",
    "#     film_level_words = {level: [] for level in levels}\n",
    "    \n",
    "#     # Добавление в словарь слов по их уровням\n",
    "#     for word in row['subs'].split():\n",
    "#         try:\n",
    "#             word_index = level_words.index(word.lower())\n",
    "#             if levels_dict['A1'] < word_index < levels_dict['A2']:\n",
    "#                 if word not in film_level_words['A1']:\n",
    "#                     film_level_words['A1'].append(word)\n",
    "#             elif levels_dict['A2'] < word_index < levels_dict['B1']:\n",
    "#                 if word not in film_level_words['A2']:\n",
    "#                     film_level_words['A2'].append(word)\n",
    "#             elif levels_dict['B1'] < word_index < levels_dict['B2']:\n",
    "#                 if word not in film_level_words['B1']:\n",
    "#                     film_level_words['B1'].append(word)\n",
    "#             elif levels_dict['B2'] < word_index < levels_dict['C1']:\n",
    "#                 if word not in film_level_words['B2']:\n",
    "#                     film_level_words['B2'].append(word)\n",
    "#             else:\n",
    "#                 if word not in film_level_words['C1']:\n",
    "#                     film_level_words['C1'].append(word)\n",
    "#         except ValueError:\n",
    "#             pass\n",
    "    \n",
    "#     # Подсчет количества слов по уровням\n",
    "#     levels_words_count = {level: len(film_level_words[level]) for level in levels}\n",
    "    \n",
    "#     # Добавление количества слов по уровням в отдельные столбцы\n",
    "#     total_words = sum(levels_words_count.values())\n",
    "#     for level in levels:\n",
    "#         df_movies.loc[df_movies['movie'] == row['movie'], level] = levels_words_count[level] / total_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e668a0",
   "metadata": {},
   "source": [
    "<a name='metric'></a>\n",
    "## 2. Выбор метрики\n",
    "[Обратно к \"Введению\"](#introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c963860",
   "metadata": {},
   "source": [
    "Имеем дело с задачей мультиклассификации. В связи с этим необходимо использовать соответствующие метрики определения качества моделей. Назначение проекта - помочь учащимся, изучающих английский язык. В связи с этим лучше всего использовать F1-меру, поскольку она сочетает в себе и точность (precision), и полноту (recall). Однако, поскольку мы имеем дело с мультиклассификацией, то необходимо воспользоваться \"Макро F1-мерой\" (деомнстрирующей среднее значение F1-меры по классам) и \"Микро F1-мерой\" (глобальная средняя F1-меры, вычисляющая сумму TP, FN и FP).\n",
    "\n",
    "Таким образом, для определения качества моделей будем использовать **f1_micro** и **f1_macro**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef910c7a",
   "metadata": {},
   "source": [
    "<a name='model'></a>\n",
    "## 3. Создание модели\n",
    "[Обратно к \"Введению\"](#introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df6432d",
   "metadata": {},
   "source": [
    "### 3. 1. Разделение выборок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2086fe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение датасета на тренировочную и тестовую выборки\n",
    "\n",
    "X = df_movies['subs']\n",
    "y = df_movies['level_num']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df_movies['level_num']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5340e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для нахождения предсказаний и демонстрации метрик F1: микро и макро\n",
    "vectorizer = TfidfVectorizer(stop_words={'english'})\n",
    "\n",
    "def print_f1_score(model):\n",
    "    pipe = Pipeline([\n",
    "        ('vectorizer', vectorizer),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    \n",
    "    # Проверка модели на тестовой выборке\n",
    "    print(f\"{model}:\")\n",
    "    print(\"\\tf1_micro: {:.4f}\".format(f1_score(y_test, y_pred, average='micro')))\n",
    "    print(\"\\tf1_macro: {:.4f}\".format(f1_score(y_test, y_pred, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444b49ac",
   "metadata": {},
   "source": [
    "### 3. 2. DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5fa1469",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel = DecisionTreeClassifier(random_state=42)\\nparams = {\\n    'model__max_depth': range(2, 21, 2),\\n    'model__criterion': ['gini', 'entropy']\\n}\\n\\npreprocessing = ColumnTransformer([\\n    ('vectorizer', vectorizer, 'subs'),\\n    ('scaler', scaler, levels),\\n])\\n\\npipe = Pipeline([\\n    ('preprocessing', preprocessing),\\n    ('model', model)\\n])\\n\\ngrid = GridSearchCV(pipe, params, scoring=scoring, refit='f1_micro')\\ngrid.fit(X_train, y_train)\\n\\nmetrics_cols = [f'mean_test_{x}' for x in scoring]\\nfinal_metrics = pd.DataFrame(grid.cv_results_)[metrics_cols].iloc[grid.best_index_]\\nprint(grid.best_estimator_)\\nprint(final_metrics)\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Использование GridSearchCV для DecisionTreeClassifier\n",
    "\n",
    "'''\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "params = {\n",
    "    'model__max_depth': range(2, 21, 2),\n",
    "    'model__criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('vectorizer', vectorizer, 'subs'),\n",
    "    ('scaler', scaler, levels),\n",
    "])\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "grid = GridSearchCV(pipe, params, scoring=scoring, refit='f1_micro')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "metrics_cols = [f'mean_test_{x}' for x in scoring]\n",
    "final_metrics = pd.DataFrame(grid.cv_results_)[metrics_cols].iloc[grid.best_index_]\n",
    "print(grid.best_estimator_)\n",
    "print(final_metrics)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f555efda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(criterion='entropy', max_depth=20, random_state=42):\n",
      "\tf1_micro: 0.6224\n",
      "\tf1_macro: 0.6300\n"
     ]
    }
   ],
   "source": [
    "tree_model = DecisionTreeClassifier(random_state=42, max_depth=20, criterion='entropy')\n",
    "print_f1_score(tree_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df939758",
   "metadata": {},
   "source": [
    "### 3. 3. RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ee4c8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(max_depth=20, n_estimators=350, random_state=42):\n",
      "\tf1_micro: 0.6327\n",
      "\tf1_macro: 0.5306\n"
     ]
    }
   ],
   "source": [
    "forest_model = RandomForestClassifier(random_state=42, n_estimators=350, max_depth=20)\n",
    "print_f1_score(forest_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f446f7",
   "metadata": {},
   "source": [
    "### 3. 4. LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15e9e1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=35, max_iter=300, random_state=42):\n",
      "\tf1_micro: 0.7449\n",
      "\tf1_macro: 0.6824\n"
     ]
    }
   ],
   "source": [
    "logreg_model = LogisticRegression(random_state=42, max_iter=300, C=35)\n",
    "print_f1_score(logreg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16df1c51",
   "metadata": {},
   "source": [
    "### 3. 5. RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2acb8aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RidgeClassifier(alpha=0.1, random_state=42):\n",
      "\tf1_micro: 0.7857\n",
      "\tf1_macro: 0.7684\n"
     ]
    }
   ],
   "source": [
    "ridge_model = RidgeClassifier(random_state=42, alpha=0.1)\n",
    "print_f1_score(ridge_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e64c999",
   "metadata": {},
   "source": [
    "### 3. 6. SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8dd4fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(l1_ratio=0.01, loss='huber', penalty='elasticnet',\n",
      "              random_state=42):\n",
      "\tf1_micro: 0.7755\n",
      "\tf1_macro: 0.7586\n"
     ]
    }
   ],
   "source": [
    "svc_model = SGDClassifier(random_state=42, loss='huber', alpha=0.0001, penalty='elasticnet', l1_ratio=0.01)\n",
    "print_f1_score(svc_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3a0637",
   "metadata": {},
   "source": [
    "### 3. 7. LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec9f82fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=3, random_state=42):\n",
      "\tf1_micro: 0.7857\n",
      "\tf1_macro: 0.7684\n"
     ]
    }
   ],
   "source": [
    "svc_model = LinearSVC(random_state=42, C=3)\n",
    "print_f1_score(svc_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396ab165",
   "metadata": {},
   "source": [
    "<a name='analysis'></a>\n",
    "## 4. Анализ результатов\n",
    "[Обратно к \"Введению\"](#introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916ebf72",
   "metadata": {},
   "source": [
    "Из 6 рассмотренных моделей наилучший результат по метрикам 'f1_micro' и 'f1_macro' продемонстрировали модели:\n",
    "- RidgeClassifier(alpha=0.1, random_state=42):\n",
    "\t- f1_micro: 0.7857\n",
    "\t- f1_macro: 0.7684\n",
    "- LinearSVC(C=3, random_state=42):\n",
    "\t- f1_micro: 0.7857\n",
    "\t- f1_macro: 0.7684\n",
    "   \n",
    "Поскольку необходимо использовать только одну модель, возьмем вторую: **LinearSVC(C=3, random_state=42)**, и сохраним полученную модель в отдельный файл для дальнейшего внедрения модели машинного обучения в веб-приложение по определению уровню английского языка, необходимого для понимания по меньшей мере 50-70% фильма."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8e8030",
   "metadata": {},
   "source": [
    "<a name='model_saving'></a>\n",
    "## 5. Сохранение модели\n",
    "[Обратно к \"Введению\"](#introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d96f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "\n",
    "svc_pipe = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('model', svc_model)\n",
    "])\n",
    "    \n",
    "svc_pipe.fit(X_train, y_train)\n",
    "\n",
    "with open('./main.pcl', 'wb') as model_file:\n",
    "    dump(svc_pipe, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b790761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subs</th>\n",
       "      <th>level_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>pay careful attention to everything you see an...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>saroo come on get up quickly hold it properly ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>itchy a few more degree to the left now tap no...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>you've reached doug sorry i missed your call p...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>and how it changed our valley forever there wa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>what give you that idea want to know the secre...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>hi todd mahar eharmony how can i help you toda...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>we keep moving on no messing around you mess t...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>michelle please don't hang up just talk to me ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>there's a moment oforderly silence before foot...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 subs  level_num\n",
       "48  pay careful attention to everything you see an...          4\n",
       "50  saroo come on get up quickly hold it properly ...          3\n",
       "5   itchy a few more degree to the left now tap no...          3\n",
       "89  you've reached doug sorry i missed your call p...          2\n",
       "7   and how it changed our valley forever there wa...          1\n",
       "26  what give you that idea want to know the secre...          3\n",
       "97  hi todd mahar eharmony how can i help you toda...          3\n",
       "51  we keep moving on no messing around you mess t...          3\n",
       "0   michelle please don't hang up just talk to me ...          3\n",
       "84  there's a moment oforderly silence before foot...          1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = svc_pipe.predict(X_test)\n",
    "predictions = pd.DataFrame(X_test)\n",
    "predictions = predictions.merge(pd.Series(y_pred).rename('level_num'), right_index=True, left_index=True)\n",
    "predictions.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724a64cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
